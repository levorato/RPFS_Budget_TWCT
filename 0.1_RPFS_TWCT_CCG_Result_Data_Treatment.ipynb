{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPFS Problem (TWCT objective) - Data treatment of result files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files in the output folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootfolder = os.getcwd()\n",
    "file_list = glob.glob(os.path.join(rootfolder, 'output') + '/*.csv', recursive=True)\n",
    "file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the CSV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative script to treat files with incorrect number of coluns or faulty lines\n",
    "def alternative_csv_reader(filename, delimiter=',', header=0, names=None):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines() \n",
    "        count = 1\n",
    "        line_list = []\n",
    "        num_columns = 20\n",
    "        for line in lines:  # Strips the newline character \n",
    "            #print(\"line{}: {}\".format(count, line.strip())) \n",
    "            nc = len(line.split(','))\n",
    "            if 'executionId,' in line:\n",
    "                #num_columns = nc\n",
    "                print('Detected {0} columns in CSV file.'.format(nc))\n",
    "            else:\n",
    "                if 'none,' in line:\n",
    "                    if nc == num_columns:\n",
    "                        line_list.append(line)\n",
    "                    elif nc > num_columns:  # treat strange truncated lines\n",
    "                        line = line[line.rfind('none,'):]\n",
    "                        nc = len(line.split(','))\n",
    "                        if nc == num_columns:\n",
    "                            print('WARN: truncating line {0}, for having more columns than expected.'.format(count))\n",
    "                            line_list.append(line)\n",
    "                        else:\n",
    "                            print('WARN: Ignoring line {0}, since it has {1} columns, instead of {2}: '.format(count, nc, num_columns), line)\n",
    "                    else:  # Ignore line\n",
    "                        print('WARN: Ignoring line {0}: '.format(count), line)    \n",
    "                elif len(line_list[-1].split(',')) < num_columns:  # current line is a continuation of the previous one\n",
    "                    line_list[-1] = line_list[-1].replace('\\n', '') + line\n",
    "                    print('*** Treated line {0}: '.format(count), line_list[-1])\n",
    "                else:  # Ignore line\n",
    "                    print('WARN: Ignoring line {0}: '.format(count), line)\n",
    "            count += 1\n",
    "        # assert all lines have the same number of columns\n",
    "        count = 1\n",
    "        for line in line_list:\n",
    "            nc = len(line.split(','))\n",
    "            if nc != num_columns:\n",
    "                print('ERROR: Line {0} has {1} columns, instead of {2}: '.format(count, nc, num_columns), line)\n",
    "            count += 1\n",
    "        text_data = StringIO(''.join(line_list))\n",
    "        #print('line_list: ', str(line_list))\n",
    "        #print('text_data: ', text_data)\n",
    "        df = pd.read_csv(text_data, delimiter=delimiter, header=header, names=names)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all CSV files and append all data to a single dataframe (one per solution method: Wilson, Wagner) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfdict = dict()\n",
    "for filepath in file_list:\n",
    "    print('Processing file ', filepath)\n",
    "    try:\n",
    "        df_ = pd.read_csv(filepath, delimiter=',', header=0, names=['executionId','ub_name','instance_name','alpha','n','m','budget_Gamma','wct','permutation','time_spent','time_to_best_sol','mp_total_time','sp_total_time','iterations','num_visited_solutions','num_improvements','is_optimal','validated','gap','lb','cost','wct_validation'])\n",
    "    except:  # try alternative method to read csv lines\n",
    "        df_ = alternative_csv_reader(filename, delimiter=',', header=0, names=['executionId','ub_name','instance_name','alpha','n','m','budget_Gamma','wct','permutation','time_spent','time_to_best_sol','mp_total_time','sp_total_time','iterations','num_visited_solutions','num_improvements','is_optimal','validated','gap','lb','cost','wct_validation'])\n",
    "    filename = filepath[filepath.rfind(os.path.sep)+1:]\n",
    "    modelname = filename[len('separation_wct_'):filename.find('_randomweights')]\n",
    "    print('Read results for model ' + modelname)\n",
    "    if modelname in dfdict:\n",
    "        dfdict[modelname] = pd.concat([dfdict[modelname], df_])\n",
    "    else:\n",
    "        dfdict[modelname] = df_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdict['manne'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated header rows from both dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_invalid_values(df):\n",
    "    all_invalid_values = set()\n",
    "    for col in df:\n",
    "        if col not in ['executionId','ub_name','instance_name','budget_Gamma','permutation','is_optimal','validated']:\n",
    "            # 'alpha','n','m','cmax','time_spent','time_to_best_sol','iterations','num_visited_solutions','num_improvements','gap','lb','cost','cmax_dp'\n",
    "            a = pd.to_numeric(df[col], errors='coerce')\n",
    "            idx = a.isna()\n",
    "            invalid_values = df.loc[idx][col].unique()\n",
    "            all_invalid_values.update(invalid_values)\n",
    "        #elif col in ['is_optimal','validated']\n",
    "    print('Invalid values:', all_invalid_values)\n",
    "    return all_invalid_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dfdict.items():\n",
    "    dfdict[key] = df[(df['executionId'] != 'executionId')]\n",
    "    print(key, dfdict[key].dtypes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def filter_invalid_values(df):\n",
    "    # IMPORTANT: AVOID FILTERING 'NAN' VALUES\n",
    "    for invalid_value in find_invalid_values(df):\n",
    "        if isinstance(invalid_value, str):  # Evita filtrar os nan\n",
    "            df = df[~(df == invalid_value).any(axis=1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for key, df in dfdict.items():\n",
    "    print(df.info())\n",
    "    df.replace('Inf', 'nan', inplace=True)\n",
    "    df = filter_invalid_values(df)\n",
    "    find_invalid_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert column types from object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_types(df):\n",
    "    for col in df:\n",
    "        if col in ['alpha','n','m','wct','budget_Gamma','time_spent','time_to_best_sol','iterations','num_visited_solutions','num_improvements','gap','lb','cost','wct_validation', 'mp_total_time', 'sp_total_time', 'seq']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        elif col in ['is_optimal','validated']:\n",
    "            df[col] = df[col].astype('bool')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for key, df in dfdict.items():\n",
    "    dfdict[key] = convert_column_types(df)\n",
    "    print(key, dfdict[key].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim existing string columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_all_columns(df):\n",
    "    \"\"\"\n",
    "    Trim whitespace from ends of each value across all series in dataframe\n",
    "    \"\"\"\n",
    "    trim_strings = lambda x: x.strip() if isinstance(x, str) else x\n",
    "    return df.applymap(trim_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dfdict.items():\n",
    "    dfdict[key] = trim_all_columns(df)\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include a column with the name of the underlying C&CG MILP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dfdict.items():\n",
    "    dfdict[key]['model'] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include a column with the name of the instance type (ying or tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dfdict.items():\n",
    "    dfdict[key]['instance_type'] = df['instance_name'].apply(lambda x: 'tail' if (x[0:5] == 'tail') else 'ying')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(list(dfdict.values()))\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix instance names \n",
    "\n",
    "The original instance names, in the instance file zip, were assembled incorrectly.\n",
    "\n",
    "The problem lies in the alpha percentage. We are now going to fix this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seq'] = df['instance_name'].apply(lambda x: x[x.find('_')-2:x.find('_')])\n",
    "df['n_str'] = df['n'].astype(str).str.zfill(3)\n",
    "df['alpha_str'] = df['alpha'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['instance_name'] = 'RB' + df['n_str'] + df['alpha_str'] + df['seq'] + '_' + df['instance_name'].apply(lambda x: x[x.find('_')+1:])\n",
    "df.drop(columns=['n_str', 'alpha_str'], inplace=True)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round columns containing time (in seconds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_spent'] = df['time_spent'].round(2)\n",
    "df['time_to_best_sol'] = df['time_to_best_sol'].round(2)\n",
    "df['mp_total_time'] = df['mp_total_time'].round(2)\n",
    "df['sp_total_time'] = df['sp_total_time'].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort data according to model, instance_name, alpha, n, m and Gamma and set index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorting dataset...')\n",
    "df = df.sort_values(['model', 'n', 'm', 'alpha', 'seq', 'budget_Gamma', 'instance_type'])\n",
    "display(df.dtypes)\n",
    "df = df.set_index(['model', 'n', 'm', 'alpha', 'seq', 'budget_Gamma', 'instance_type'])\n",
    "display(df.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find missing results, for a given value of alpha, n and m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given group of alpha, n, m and budget_Gamma, there should be 10 results.\n",
    "\n",
    "First we will build a dataframe with the instances list and all required budget values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "rootfolder = os.getcwd()\n",
    "file_list = Path(os.path.join(rootfolder, 'instances', 'robust')).rglob('*.txt')\n",
    "file_set = set()\n",
    "for path in file_list:\n",
    "    instance_path = path.name\n",
    "    #print('instance_path: ' + instance_path)    \n",
    "    if '.txt' not in instance_path:\n",
    "        continue\n",
    "    if 'tail' in instance_path:\n",
    "        if instance_path[:instance_path.find('_')] not in ['tail001', 'tail002', 'tail003', 'tail004', 'tail005', 'tail006', 'tail007', 'tail008', 'tail009', 'tail010']:\n",
    "            #print(instance_path[:instance_path.find('_')])\n",
    "            continue\n",
    "    instance_name = instance_path[instance_path.rfind(os.path.sep)+1:]\n",
    "    file_set.add(instance_name)\n",
    "print(file_set, file_set)\n",
    "for instance_name in file_set:\n",
    "    #print('instance_name: ' + instance_name)\n",
    "    seq = instance_name[instance_name.find('_')-2:instance_name.find('_')]\n",
    "    info = instance_name[instance_name.find('_')+1:]\n",
    "    n = info[:info.find('_')]\n",
    "    info = info[info.find('_')+1:]\n",
    "    m = info[:info.find('_')]\n",
    "    info = info[info.find('_')+1:]\n",
    "    alpha = info[:info.find('_')]\n",
    "    instance_type = 'ying'\n",
    "    if 'tail' in instance_name:\n",
    "        instance_type = 'tail'\n",
    "    else:\n",
    "        if int(n) > 20:\n",
    "            continue\n",
    "    for gamma in [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "        budget_gamma = int(gamma) * (int(m) * int(n)) / 100.0\n",
    "        for model in list(dfdict.keys()):\n",
    "            data.append([model, seq, alpha, int(n), int(m), budget_gamma, instance_type])\n",
    "df_instances = pd.DataFrame(data, columns=['model', 'seq', 'alpha', 'n', 'm', 'budget_Gamma', 'instance_type'])\n",
    "for col in df_instances:\n",
    "    if col in ['alpha','n','m','budget_Gamma']:\n",
    "        df_instances[col] = pd.to_numeric(df_instances[col], errors='coerce')\n",
    "display(df_instances.dtypes)\n",
    "df_instances = df_instances.set_index(['model', 'n', 'm', 'alpha', 'seq', 'budget_Gamma', 'instance_type'])\n",
    "display(df_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instances.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets join the instances dataframe with the results one (left join)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_instances.join(df, how='left')\n",
    "df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.reset_index()[['model', 'n', 'm', 'alpha', 'seq', 'budget_Gamma', 'instance_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will export to CSV a list with all rows with NaN values (missing experimental results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = df_joined[df_joined.isnull().any(axis=1)].reset_index()[['model', 'n', 'm', 'alpha', 'seq', 'budget_Gamma', 'instance_type']]\n",
    "outputfolder = os.path.join(os.getcwd(), 'results', 'consolidated')\n",
    "if not os.path.exists(outputfolder):\n",
    "    os.makedirs(outputfolder)\n",
    "print('Saving file on folder: ' + outputfolder)\n",
    "fname = os.path.join(outputfolder, 'RPFS_TWCT_missing_results.csv')\n",
    "missing_df.to_csv(fname, sep=';')\n",
    "print('Saved: ' + fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(['alpha', 'n', 'm', 'budget_Gamma']).agg({'executionId' : ['count']}).reset_index()\n",
    "df_grouped.columns = [ ' '.join(str(i) for i in col) for col in df_grouped.columns]\n",
    "#df_grouped.reset_index(inplace=True)\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(df, values='executionId', index=['alpha', 'n'], columns=['budget_Gamma'], aggfunc='count', fill_value=0)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
    "    display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the dataset to CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "outputfolder = os.path.join(os.getcwd(), 'results', 'consolidated')\n",
    "print('Saving file on folder: ' + outputfolder)\n",
    "fname = os.path.join(outputfolder, 'RPFS_TWCT_all_results.csv')\n",
    "df.to_csv(fname, sep=';')\n",
    "print('Saved: ' + fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
