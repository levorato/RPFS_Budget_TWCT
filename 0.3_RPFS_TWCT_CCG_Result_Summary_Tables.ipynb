{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2RPFS Problem (TWCT objective) - Tables and Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this, notebook, please run notebook 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "import matplotlib.style as style\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import BoxStyle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyle_tuple = [\n",
    "     ('dotted',                (0, (1, 1))),\n",
    "     ('dashed',                (0, (5, 5))),\n",
    "     ('densely dashed',        (0, (5, 1))),\n",
    "     ('dashdotdotted',         (0, (3, 5, 1, 5, 1, 5))),\n",
    "     ('densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1))),\n",
    "\n",
    "     ('dashdotted',            (0, (3, 5, 1, 5))),\n",
    "     ('densely dashdotted',    (0, (3, 1, 1, 1))),\n",
    "     \n",
    "     ('loosely dashed',        (0, (5, 10))),\n",
    "     ('loosely dashdotted',    (0, (3, 10, 1, 10))),\n",
    "     \n",
    "\n",
    "     ('loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10))),\n",
    "     ('densely dotted',        (0, (1, 1))),\n",
    "     ('loosely dotted',        (0, (1, 10)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51483901/is-there-a-way-to-extend-the-solid-color-background-to-the-full-width-of-the-pag\n",
    "class ExtendedTextBox(BoxStyle._Base):\n",
    "    \"\"\"\n",
    "    An Extended Text Box that expands to the axes limits \n",
    "                        if set in the middle of the axes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pad=0.3, width=500.):\n",
    "        \"\"\"\n",
    "        width: \n",
    "            width of the textbox. \n",
    "            Use `ax.get_window_extent().width` \n",
    "                   to get the width of the axes.\n",
    "        pad: \n",
    "            amount of padding (in vertical direction only)\n",
    "        \"\"\"\n",
    "        self.width=width\n",
    "        self.pad = pad\n",
    "        super(ExtendedTextBox, self).__init__()\n",
    "\n",
    "    def transmute(self, x0, y0, width, height, mutation_size):\n",
    "        \"\"\"\n",
    "        x0 and y0 are the lower left corner of original text box\n",
    "        They are set automatically by matplotlib\n",
    "        \"\"\"\n",
    "        # padding\n",
    "        pad = mutation_size * self.pad\n",
    "\n",
    "        # we add the padding only to the box height\n",
    "        height = height + 2.*pad\n",
    "        # boundary of the padded box\n",
    "        y0 = y0 - pad\n",
    "        y1 = y0 + height\n",
    "        _x0 = x0\n",
    "        x0 = _x0 +width /2. - self.width/2.\n",
    "        x1 = _x0 +width /2. + self.width/2.\n",
    "\n",
    "        cp = [(x0, y0),\n",
    "              (x1, y0), (x1, y1), (x0, y1),\n",
    "              (x0, y0)]\n",
    "\n",
    "        com = [Path.MOVETO,\n",
    "               Path.LINETO, Path.LINETO, Path.LINETO,\n",
    "               Path.CLOSEPOLY]\n",
    "\n",
    "        path = Path(cp, com)\n",
    "\n",
    "        return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files in the result folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultfolder = os.path.join(os.getcwd(), 'results', 'consolidated')\n",
    "rpfs_file = os.path.join(resultfolder, 'RPFS_TWCT_all_results.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the output folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfolder = os.path.join(os.getcwd(), 'results', 'consolidated')\n",
    "outputfolder_graph = os.path.join(os.getcwd(), 'results', 'consolidated', 'graphs')\n",
    "outputfolder_table = os.path.join(os.getcwd(), 'results', 'consolidated', 'tables')\n",
    "if not os.path.exists(outputfolder_graph):\n",
    "    os.makedirs(outputfolder_graph)\n",
    "if not os.path.exists(outputfolder_table):\n",
    "    os.makedirs(outputfolder_table)\n",
    "#print('Saving files on folder: ' + outputfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process consolidated CSV result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs = pd.read_pickle(rpfs_file)  # Robust PFSP Budget solutions only\n",
    "df_rpfs.drop(columns=['executionId'], inplace=True)\n",
    "df_rpfs = df_rpfs.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Robust dataframe: calculating new fields.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs['optimal'] = df_rpfs['is_optimal'] & df_rpfs['validated'] & (df_rpfs['gap'] <= 1e-8)\n",
    "df_rpfs['time_limit'] = 7200.0\n",
    "df_rpfs['time_limit_2'] = 7200.0 * 2\n",
    "df_rpfs['mp_total_time'] = (df_rpfs['n'] < 15).astype(int) * np.minimum(df_rpfs['mp_total_time'], df_rpfs['time_limit']) + (df_rpfs['n'] >= 15).astype(int) * np.minimum(df_rpfs['mp_total_time'], df_rpfs['time_limit_2'])\n",
    "df_rpfs['time'] = df_rpfs['mp_total_time'] + df_rpfs['sp_total_time']\n",
    "df_rpfs['gap'] = df_rpfs['gap'] * 100.0\n",
    "df_rpfs['RobCost_worstcase'] = df_rpfs['wct_validation']\n",
    "df_rpfs = df_rpfs.rename(columns={\"budget_Gamma\": \"RobCost_Gamma\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Robust PFSP Budget solutions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace model names with the name used in table presentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs = df_rpfs[(df_rpfs['model'] != 'hybrid')]\n",
    "df_rpfs['model'].replace({'hybrid-liao-you': 'Liao-You-Hybrid', 'hybrid-wilson': 'Wilson-Hybrid', 'liao-you': 'Liao-You', 'manne': 'Manne',\n",
    "                    'tba': 'TBA', 'ts2': 'TS2', 'ts3': 'TS3', 'wagner-wst2': 'WST2', 'wilson': 'Wilson'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain list of C&CG models, instance types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = df_rpfs['model'].unique().tolist()\n",
    "instance_type_list = df_rpfs['instance_type'].unique().tolist()\n",
    "print(model_list)\n",
    "print(instance_type_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column containing the instance size as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_rpfs\n",
    "(df_temp['n'].astype(str) + 'x' + df_temp['m'].astype(str)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_rpfs\n",
    "df_temp['instance_size'] = df_temp['n'].astype(str) + 'x' + df_temp['m'].astype(str)\n",
    "df_rpfs = df_temp.set_index(['model', 'n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type'])\n",
    "df_rpfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating errors in the `gap` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs['gap'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df_rpfs.reset_index()[['model', 'n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_name', 'gap', 'wct', 'RobCost_worstcase', 'lb']]\n",
    "df_check[(df_check['gap'] < -1e-5)].to_csv(os.path.join(os.getcwd(), 'results', 'negative_gap_list.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs['gap'] = df_rpfs['gap'].apply(lambda x: np.maximum(x, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs['gap'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2. Performance given all instances \n",
    "\n",
    "Model-wise Robust PFSP C&CG performance comparison, given all instances.\n",
    "\n",
    "* % Best Performance is the percentage of instances solved to optimality where the model achieved shorter execution time, when compared to the other models; \n",
    "\n",
    "* % Solved contains the percentage of instances solved within the time limit; \n",
    "\n",
    "* % Solved < n x m > represents the percentage of solved instances of size n x m; \n",
    "\n",
    "* Avg. % Gap is the average percentage gap of solutions from instances not solved to optimality; \n",
    "\n",
    "* Median time is the median execution time, in seconds; \n",
    "\n",
    "* Median iterations is the median of the number of iterations performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_rpfs.reset_index()\n",
    "df_model = df_model[df_model['model'] == 'hybrid']\n",
    "df_model = df_model[df_model['optimal'] == True]    \n",
    "df_model = df_model.set_index(['n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type'])\n",
    "\n",
    "df_others = df_rpfs.reset_index()\n",
    "df_others = df_others[df_others['model'] != 'hybrid']\n",
    "df_others = df_others[df_others['optimal'] == True] \n",
    "group_columns = ['n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type']\n",
    "df_best_performance = df_others[group_columns + ['time']].groupby(by=group_columns).min()['time']\n",
    "df_best_performance = df_best_performance.to_frame()\n",
    "df_best_performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_compare = df_best_performance.join(df_model, how='inner', \n",
    "                                                     on=['n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type'],\n",
    "                                                     lsuffix='_best')\n",
    "df_compare['time_wins'] = (df_compare['time'] < df_compare['time_best']).astype(int)\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perc_best_performance(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    df_model = df_model[df_model['optimal'] == True]    \n",
    "    df_model = df_model.set_index(['n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type'])\n",
    "    if len(df_model.index) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    df_others = df.reset_index()\n",
    "    df_others = df_others[df_others['model'] != model]\n",
    "    df_others = df_others[df_others['optimal'] == True] \n",
    "    group_columns = ['n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type']\n",
    "    df_best_performance = df_others[group_columns + ['time']].groupby(by=group_columns).min()['time']\n",
    "    df_best_performance = df_best_performance.to_frame()\n",
    "    if len(df_best_performance.index) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    df_compare = df_best_performance.join(df_model, how='inner', \n",
    "                                                     on=group_columns,\n",
    "                                                     lsuffix='_best')\n",
    "    df_compare['time_wins'] = (df_compare['time'] < df_compare['time_best']).astype(int)\n",
    "    return np.round(100.0 * df_compare['time_wins'].sum() / len(df_compare.index), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perc_solved(df, model, instance_type = None, instance_size = None):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    df_ = df_model\n",
    "    if instance_type is not None:\n",
    "        df_ = df_[df_['instance_type'] == instance_type]\n",
    "    if instance_size is not None:\n",
    "        df_ = df_[df_['instance_size'] == instance_size]\n",
    "    if len(df_.index) > 0:\n",
    "        return np.round(100.0 * len(df_[(df_['optimal'] == True)].index) / len(df_.index), 2)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg. % Gap is the average percentage gap of solutions from instances not solved to optimality\n",
    "def calculate_avg_perc_gap(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    df_model = df_model[df_model['optimal'] == False]\n",
    "    return np.round(df_model['gap'].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_median_time(df, model, time_col_name):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model[time_col_name].median(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_time(df, model, time_col_name):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model[time_col_name].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_std_time(df, model, time_col_name):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model[time_col_name].std(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_median_iterations(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model['iterations'].median(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_iterations(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model['iterations'].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_std_iterations(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model['iterations'].std(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = dict()\n",
    "for model in model_list:\n",
    "    model_stats[model] = dict()\n",
    "    model_stats[model]['% Best Performance'] = calculate_perc_best_performance(df_rpfs, model)\n",
    "    model_stats[model]['% Solved'] = calculate_perc_solved(df_rpfs, model)  # given all instances\n",
    "    for instance_type in instance_type_list:  # group by instance type and size\n",
    "        df_itype = df_rpfs.reset_index()\n",
    "        df_itype = df_itype[(df_itype['instance_type'] == instance_type)]\n",
    "        instance_size_list = ['10x2', '10x3', '10x4', '10x5', '15x5']  # df_itype['instance_size'].unique().tolist()\n",
    "        for instance_size in instance_size_list:\n",
    "            model_stats[model]['% Solved '+ instance_size] = calculate_perc_solved(df_rpfs, model, instance_type, instance_size)\n",
    "    model_stats[model]['Avg. % gap'] = calculate_avg_perc_gap(df_rpfs, model)\n",
    "    model_stats[model]['Median time'] = calculate_median_time(df_rpfs, model, 'time')\n",
    "    model_stats[model]['Median time MP'] = calculate_median_time(df_rpfs, model, 'mp_total_time')\n",
    "    model_stats[model]['Median time SP'] = calculate_median_time(df_rpfs, model, 'sp_total_time')\n",
    "    model_stats[model]['Median iterations'] = calculate_median_iterations(df_rpfs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats_df = pd.DataFrame.from_dict(model_stats)\n",
    "model_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to Tableau, after melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats_df.transpose().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_vars = model_stats_df.transpose().columns\n",
    "df_melt_model_stats_df = pd.melt(model_stats_df.transpose().reset_index(), id_vars=['index'], value_vars=value_vars)\n",
    "df_melt_model_stats_df['Model'] = df_melt_model_stats_df['index']\n",
    "df_melt_model_stats_df.to_excel(os.path.join(outputfolder_table, 'twct_model_stats.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output table as HTML\n",
    "pd.set_option('colheader_justify', 'center')   # FOR TABLE <th>\n",
    "\n",
    "html_string = '''\n",
    "<html>\n",
    "  <head><title>HTML Pandas Dataframe with CSS</title></head>\n",
    "  <link rel=\"stylesheet\" type=\"text/css\" href=\"df_style.css\"/>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "# OUTPUT AN HTML FILE\n",
    "with open(os.path.join(outputfolder_table, 'model_stats.html'), 'w') as f:\n",
    "    f.write(html_string.format(table=model_stats_df.to_html(classes='mystyle')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3. Performance per instance group and model\n",
    "\n",
    "Model-wise Robust PFSP C&CG performance comparison, per instance group.\n",
    "\n",
    "* % Best Performance is the percentage of instances solved to optimality where the model achieved shorter execution time, when compared to the other models; \n",
    "\n",
    "* % Solved contains the percentage of instances solved within the time limit; \n",
    "\n",
    "* Avg. % Gap is the average percentage gap of solutions from instances not solved to optimality; \n",
    "\n",
    "* Avg. time and Std. dev. of time are the mean and standard deviation in solution time (s), respectively;\n",
    "\n",
    "* Avg. iterations and Std. dev. of iterations are the mean and standard deviation of the number of iterations performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_itype['instance_size'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's create a table only with non-hybrid solution methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_instance_stats = dict()\n",
    "for instance_type in instance_type_list:  # group by instance type and size\n",
    "    df_base = df_rpfs.reset_index()\n",
    "    exclude_model_list = ['Wilson-Hybrid', 'Liao-You-Hybrid']\n",
    "    df_base = df_base[~(df_base['model'].isin(exclude_model_list))]\n",
    "    model_list_reduced = [_ for _ in model_list if _ not in exclude_model_list]\n",
    "    df_itype = df_base\n",
    "    df_itype = df_itype[(df_itype['instance_type'] == instance_type)]\n",
    "    instance_size_list = ['10x2', '10x3', '10x4', '10x5', '15x5']  # df_itype['instance_size'].unique().tolist()\n",
    "    for instance_size in instance_size_list:\n",
    "        df_instance = df_itype[df_itype['instance_size'] == instance_size]\n",
    "        for model in model_list_reduced:\n",
    "            per_instance_stats[(instance_type,instance_size,model)] = dict()\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% Best Performance'] = calculate_perc_best_performance(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% Solved'] = calculate_perc_solved(df_base, model, instance_type, instance_size)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. % gap'] = calculate_avg_perc_gap(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. time'] = calculate_avg_time(df_instance, model, 'time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of time'] = calculate_std_time(df_instance, model, 'time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. MP time'] = calculate_avg_time(df_instance, model, 'mp_total_time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. SP time'] = calculate_avg_time(df_instance, model, 'sp_total_time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. iterations'] = calculate_avg_iterations(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of iterations'] = calculate_std_iterations(df_instance, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57606801/pandas-style-options-to-latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y, z) for (x, y, z) in per_instance_stats.keys() if (x == 'ying' and y in ['10x2', '10x3'])]\n",
    "per_instance_stats1 = { your_key: per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table3a = pd.DataFrame.from_dict(per_instance_stats1)\n",
    "df_table3a.columns = df_table3a.columns.droplevel()\n",
    "df_table3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y, z) for (x, y, z) in per_instance_stats.keys() if (x == 'ying' and y in ['10x4', '10x5'])]\n",
    "per_instance_stats2 = { your_key: per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table3b = pd.DataFrame.from_dict(per_instance_stats2)\n",
    "df_table3b.columns = df_table3b.columns.droplevel()\n",
    "df_table3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export table to Tableau, after melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table3 = pd.DataFrame.from_dict(per_instance_stats)\n",
    "df_table3.columns = df_table3.columns.droplevel()\n",
    "value_vars = df_table3.transpose().columns\n",
    "df_melt_table3 = pd.melt(df_table3.transpose().reset_index(), id_vars=['level_0', 'level_1'], value_vars=value_vars)\n",
    "df_melt_table3['Instance size'] = df_melt_table3['level_0']\n",
    "df_melt_table3['Model'] = df_melt_table3['level_1']\n",
    "df_melt_table3.to_excel(os.path.join(outputfolder_table, 'twct_table3.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output table as HTML\n",
    "pd.set_option('colheader_justify', 'center')   # FOR TABLE <th>\n",
    "\n",
    "html_string = '''\n",
    "<html>\n",
    "  <head><title>HTML Pandas Dataframe with CSS</title></head>\n",
    "  <link rel=\"stylesheet\" type=\"text/css\" href=\"df_style.css\"/>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "# OUTPUT AN HTML FILE\n",
    "with open(os.path.join(outputfolder_table, 'instance_stats_1.html'), 'w') as f:\n",
    "    f.write(html_string.format(table=df_table3a.to_html(classes='mystyle')))\n",
    "with open(os.path.join(outputfolder_table, 'instance_stats_2.html'), 'w') as f:\n",
    "    f.write(html_string.format(table=df_table3b.to_html(classes='mystyle')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(df_table3.to_latex(index=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we'll create a table with hybrid and non-hybrid solution methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_instance_stats = dict()\n",
    "for instance_type in instance_type_list:  # group by instance type and size\n",
    "    df_itype = df_rpfs.reset_index()\n",
    "    df_itype = df_itype[(df_itype['instance_type'] == instance_type)]\n",
    "    instance_size_list = ['10x2', '10x3', '10x4', '10x5', '15x5']  # df_itype['instance_size'].unique().tolist()\n",
    "    for instance_size in instance_size_list:\n",
    "        df_instance = df_itype[df_itype['instance_size'] == instance_size]\n",
    "        for model in model_list:\n",
    "            per_instance_stats[(instance_type,instance_size,model)] = dict()\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% Best Performance'] = calculate_perc_best_performance(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% Solved'] = calculate_perc_solved(df_rpfs, model, instance_type, instance_size)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. % gap'] = calculate_avg_perc_gap(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. time'] = calculate_avg_time(df_instance, model, 'time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of time'] = calculate_std_time(df_instance, model, 'time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. MP time'] = calculate_avg_time(df_instance, model, 'mp_total_time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. SP time'] = calculate_avg_time(df_instance, model, 'sp_total_time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. iterations'] = calculate_avg_iterations(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of iterations'] = calculate_std_iterations(df_instance, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57606801/pandas-style-options-to-latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y, z) for (x, y, z) in per_instance_stats.keys() if (x == 'ying' and y in ['10x2', '10x3', '10x4'])]\n",
    "per_instance_stats1 = { your_key: per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table3a = pd.DataFrame.from_dict(per_instance_stats1)\n",
    "df_table3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y, z) for (x, y, z) in per_instance_stats.keys() if (x == 'ying' and y in ['10x5', '15x5'])]\n",
    "per_instance_stats2 = { your_key: per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table3b = pd.DataFrame.from_dict(per_instance_stats2)\n",
    "df_table3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output table as HTML\n",
    "pd.set_option('colheader_justify', 'center')   # FOR TABLE <th>\n",
    "\n",
    "html_string = '''\n",
    "<html>\n",
    "  <head><title>HTML Pandas Dataframe with CSS</title></head>\n",
    "  <link rel=\"stylesheet\" type=\"text/css\" href=\"df_style.css\"/>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "# OUTPUT AN HTML FILE\n",
    "with open(os.path.join(outputfolder_table, 'instance_stats_1.html'), 'w') as f:\n",
    "    f.write(html_string.format(table=df_table3a.to_html(classes='mystyle')))\n",
    "with open(os.path.join(outputfolder_table, 'instance_stats_2.html'), 'w') as f:\n",
    "    f.write(html_string.format(table=df_table3b.to_html(classes='mystyle')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(df_table3.to_latex(index=True)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
