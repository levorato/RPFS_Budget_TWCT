{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2RPFS Problem (TWCT objective) - Validate individual solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this, notebook, please run notebook 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "import matplotlib.style as style\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import BoxStyle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files in the result folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultfolder = os.path.join(os.getcwd(), 'results', 'consolidated')\n",
    "rpfs_file = os.path.join(resultfolder, 'RPFS_TWCT_all_results.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the output folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfolder = os.path.join(os.getcwd(), 'results', 'consolidated')\n",
    "outputfolder_graph = os.path.join(os.getcwd(), 'results', 'consolidated', 'graphs')\n",
    "outputfolder_table = os.path.join(os.getcwd(), 'results', 'consolidated', 'tables')\n",
    "if not os.path.exists(outputfolder_graph):\n",
    "    os.makedirs(outputfolder_graph)\n",
    "if not os.path.exists(outputfolder_table):\n",
    "    os.makedirs(outputfolder_table)\n",
    "#print('Saving files on folder: ' + outputfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process consolidated CSV result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs = pd.read_pickle(rpfs_file)  # Robust PFSP Budget solutions only\n",
    "df_rpfs.drop(columns=['executionId'], inplace=True)\n",
    "df_rpfs = df_rpfs.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Robust dataframe: calculating new fields.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs['optimal'] = df_rpfs['is_optimal'] & df_rpfs['validated'] & (df_rpfs['gap'] <= 1e-8)\n",
    "df_rpfs['time_limit'] = 7200.0\n",
    "df_rpfs['time_limit_2'] = 7200.0 * 2\n",
    "df_rpfs['mp_total_time'] = (df_rpfs['n'] < 15).astype(int) * np.minimum(df_rpfs['mp_total_time'], df_rpfs['time_limit']) + (df_rpfs['n'] >= 15).astype(int) * np.minimum(df_rpfs['mp_total_time'], df_rpfs['time_limit_2'])\n",
    "df_rpfs['time'] = df_rpfs['mp_total_time'] + df_rpfs['sp_total_time']\n",
    "df_rpfs['gap'] = df_rpfs['gap'] * 100.0\n",
    "df_rpfs['RobCost_worstcase'] = df_rpfs['wct_validation']\n",
    "df_rpfs = df_rpfs.rename(columns={\"budget_Gamma\": \"RobCost_Gamma\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Robust PFSP Budget solutions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace model names with the name used in table presentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs = df_rpfs[(df_rpfs['model'] != 'hybrid')]\n",
    "df_rpfs['model'].replace({'hybrid-liao-you': 'Liao-You-Hybrid', 'hybrid-wilson': 'Wilson-Hybrid', 'hybrid-manne': 'Manne-Hybrid', \n",
    "                          'liao-you': 'Liao-You', 'manne': 'Manne',\n",
    "                          'tba': 'TBA', 'ts2': 'TS2', 'ts3': 'TS3', 'wagner-wst2': 'WST2', 'wilson': 'Wilson'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain list of C&CG models, instance types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = df_rpfs['model'].unique().tolist()\n",
    "instance_type_list = df_rpfs['instance_type'].unique().tolist()\n",
    "print(model_list)\n",
    "print(instance_type_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column containing the instance size as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_rpfs\n",
    "(df_temp['n'].astype(str) + 'x' + df_temp['m'].astype(str)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_rpfs\n",
    "df_temp['instance_size'] = df_temp['n'].astype(str) + 'x' + df_temp['m'].astype(str)\n",
    "df_rpfs = df_temp.set_index(['model', 'n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type'])\n",
    "df_rpfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating errors in the `gap` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs['gap'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df_rpfs.reset_index()[['model', 'n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_name', 'gap', 'wct', 'RobCost_worstcase', 'lb']]\n",
    "df_check[(df_check['gap'] < -1e-5)].to_csv(os.path.join(os.getcwd(), 'results', 'negative_gap_list.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs['gap'] = df_rpfs['gap'].apply(lambda x: np.maximum(x, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpfs['gap'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perc_best_performance(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    df_model = df_model[df_model['optimal'] == True]    \n",
    "    df_model = df_model.set_index(['n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type'])\n",
    "    if len(df_model.index) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    df_others = df.reset_index()\n",
    "    df_others = df_others[df_others['model'] != model]\n",
    "    df_others = df_others[df_others['optimal'] == True] \n",
    "    group_columns = ['n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type']\n",
    "    df_best_performance = df_others[group_columns + ['time']].groupby(by=group_columns).min()['time']\n",
    "    df_best_performance = df_best_performance.to_frame()\n",
    "    if len(df_best_performance.index) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    df_compare = df_best_performance.join(df_model, how='inner', \n",
    "                                                     on=group_columns,\n",
    "                                                     lsuffix='_best')\n",
    "    df_compare['time_wins'] = (df_compare['time'] < df_compare['time_best']).astype(int)\n",
    "    return np.round(100.0 * df_compare['time_wins'].sum() / len(df_compare.index), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perc_solved(df, model, instance_type = None, instance_size = None, alpha = None):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    df_ = df_model\n",
    "    if instance_type is not None:\n",
    "        df_ = df_[df_['instance_type'] == instance_type]\n",
    "    if instance_size is not None:\n",
    "        df_ = df_[df_['instance_size'] == instance_size]\n",
    "    if alpha is not None:\n",
    "        df_ = df_[df_['alpha'] == alpha]\n",
    "    if len(df_.index) > 0:\n",
    "        return np.round(100.0 * len(df_[(df_['optimal'] == True)].index) / len(df_.index), 2)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg. % Gap is the average percentage gap of solutions from instances not solved to optimality\n",
    "def calculate_avg_perc_gap(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    df_model = df_model[df_model['optimal'] == False]\n",
    "    if df_model['gap'].mean() >= 1e-2:\n",
    "        return np.round(df_model['gap'].mean(), 2)\n",
    "    else:\n",
    "        return df_model['gap'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_median_time(df, model, time_col_name):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model[time_col_name].median(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_time(df, model, time_col_name):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model[time_col_name].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_std(df, model, col_name):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    if col_name == 'gap':\n",
    "        df_model = df_model[df_model['optimal'] == False]\n",
    "    if df_model[col_name].std() >= 1e-2:\n",
    "        return np.round(df_model[col_name].std(), 2)\n",
    "    else:\n",
    "        return df_model[col_name].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(df, model, col_name, confidence=0.95):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    if col_name == 'gap':\n",
    "        df_model = df_model[df_model['optimal'] == False]\n",
    "    data = df_model[col_name]\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    lb = np.round(m-h, 2)\n",
    "    ub = np.round(m+h, 2)\n",
    "    if np.isnan(lb) or np.isnan(ub):\n",
    "        return '-'\n",
    "    return '[{}, {}]'.format(lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_median_iterations(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model['iterations'].median(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_iterations(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    return np.round(df_model['iterations'].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_instances(df, model):\n",
    "    df_model = df.reset_index()\n",
    "    df_model = df_model[df_model['model'] == model]\n",
    "    return len(df_model.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's determine the baseline results by obtaining the smallest valid objective values among all executed solutions methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = df_rpfs.copy().reset_index()\n",
    "### df_baseline = df_baseline[(df_baseline['optimal'] == True) & (df_baseline['validated'] == True)]\n",
    "exclude_model_list = ['Wilson-Hybrid', 'Liao-You-Hybrid', 'Manne-Hybrid', 'Wilson']\n",
    "df_baseline = df_baseline[~(df_baseline['model'].isin(exclude_model_list))]\n",
    "df_baseline = df_baseline[(df_baseline['validated'] == True) & (df_baseline['wct_validation'] > 0)]\n",
    "group_key = ['n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type']  #   , 'ub_name', 'instance_name']\n",
    "df_baseline_opt_value = df_baseline.groupby(by=group_key)[['wct_validation']].min().reset_index()\n",
    "display(df_baseline_opt_value)\n",
    "merge_key = group_key + ['wct_validation']\n",
    "df_baseline_opt = df_baseline.merge(df_baseline_opt_value, on=merge_key, how='inner')\n",
    "df_baseline_opt = df_baseline_opt.drop_duplicates(subset=merge_key, keep='first').sort_values(by=merge_key)\n",
    "display(df_baseline_opt)\n",
    "df_baseline_opt.to_csv(os.path.join(outputfolder_table, 'BestOptimalSolutions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perc_valid_solutions(df, model, instance_type, instance_size):\n",
    "    df_base = df.reset_index()\n",
    "    df_base = df_base[(df_base['instance_type'] == instance_type)]\n",
    "    df_base = df_base[(df_base['instance_size'] == instance_size)]\n",
    "    df_model = df_base[df_base['model'] == model]\n",
    "    df_model_opt = df_model[(df_model['optimal'] == True) & (df_model['validated'] == True)]\n",
    "    df_join = df_baseline_opt.merge(df_model_opt, on=group_key, how='inner')\n",
    "    num_solutions = len(df_join.index)\n",
    "    df_join['obj_diff'] = df_join['wct_validation_y'] - df_join['wct_validation_x']\n",
    "    EPS = 1e-3\n",
    "    df_join['obj_less_than'] = df_join['obj_diff'] < -EPS\n",
    "    df_join['obj_greater_than'] = df_join['obj_diff'] > EPS\n",
    "    df_join['is_valid'] = ((abs(df_join['obj_diff']) <= EPS) | ( df_join['obj_less_than'] & (df_join['permutation_x'].str.strip() == df_join['permutation_y'].str.strip())))\n",
    "    df_join.to_csv(os.path.join(outputfolder_table, 'ValidSolutions_{}_{}_{}.csv'.format(model, instance_type, instance_size)))\n",
    "    df_valid = df_join[df_join['is_valid']]\n",
    "    if num_solutions > 0:\n",
    "        return 100.0 * len(df_valid.index) / num_solutions\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First, let's create tables only with non-hybrid solution methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table A1. # of correct solutions (Wilson-baseline) and performance per instance group and model\n",
    "\n",
    "Model-wise Robust PFSP C&CG performance comparison, per instance group.\n",
    "\n",
    "* % Best Performance is the percentage of instances solved to optimality where the model achieved shorter execution time, when compared to the other models; \n",
    "\n",
    "* % Solved contains the percentage of instances solved within the time limit; \n",
    "\n",
    "* Avg. % Gap is the average percentage gap of solutions from instances not solved to optimality; \n",
    "\n",
    "* Avg. time and Std. dev. of time are the mean and standard deviation in solution time (s), respectively;\n",
    "\n",
    "* Avg. iterations and Std. dev. of iterations are the mean and standard deviation of the number of iterations performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's assume Wilson model as baseline (i.e. its optimal solutions are correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_instance_stats = dict()\n",
    "for instance_type in instance_type_list:  # group by instance type and size\n",
    "    df_base = df_rpfs.reset_index()\n",
    "    exclude_model_list = ['Wilson-Hybrid', 'Liao-You-Hybrid', 'Manne-Hybrid']\n",
    "    df_base = df_base[~(df_base['model'].isin(exclude_model_list))]\n",
    "    model_list_reduced = [_ for _ in model_list if _ not in exclude_model_list]\n",
    "    df_itype = df_base\n",
    "    df_itype = df_itype[(df_itype['instance_type'] == instance_type)]\n",
    "    instance_size_list = ['10x2', '10x3', '10x4', '10x5', '15x5']  # df_itype['instance_size'].unique().tolist()\n",
    "    for instance_size in instance_size_list:\n",
    "        df_instance = df_itype[df_itype['instance_size'] == instance_size]\n",
    "        for model in model_list_reduced:\n",
    "            per_instance_stats[(instance_type,instance_size,model)] = dict()\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% Best Performance'] = calculate_perc_best_performance(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% Solved'] = calculate_perc_solved(df_base, model, instance_type, instance_size)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. % gap'] = calculate_avg_perc_gap(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of % gap'] = calculate_std(df_instance, model, 'gap')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['95% CI of % gap'] = mean_confidence_interval(df_instance, model, 'gap')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. time'] = calculate_avg_time(df_instance, model, 'time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of time'] = calculate_std(df_instance, model, 'time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. MP time'] = calculate_avg_time(df_instance, model, 'mp_total_time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. SP time'] = calculate_avg_time(df_instance, model, 'sp_total_time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. iterations'] = calculate_avg_iterations(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of iterations'] = calculate_std(df_instance, model, 'iterations')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['# instances solved'] = calculate_num_instances(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% valid solutions'] = calculate_perc_valid_solutions(df_base, model, instance_type, instance_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57606801/pandas-style-options-to-latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y, z) for (x, y, z) in per_instance_stats.keys() if (x == 'ying' and y in ['10x2', '10x3'])]\n",
    "per_instance_stats1 = { your_key: per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table3a = pd.DataFrame.from_dict(per_instance_stats1)\n",
    "df_table3a.columns = df_table3a.columns.droplevel()\n",
    "df_table3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y, z) for (x, y, z) in per_instance_stats.keys() if (x == 'ying' and y in ['10x4', '10x5'])]\n",
    "per_instance_stats2 = { your_key: per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table3b = pd.DataFrame.from_dict(per_instance_stats2)\n",
    "df_table3b.columns = df_table3b.columns.droplevel()\n",
    "df_table3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export table to Tableau, after melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table3 = pd.DataFrame.from_dict(per_instance_stats)\n",
    "df_table3.columns = df_table3.columns.droplevel()\n",
    "value_vars = df_table3.transpose().columns\n",
    "df_melt_table3 = pd.melt(df_table3.transpose().reset_index(), id_vars=['level_0', 'level_1'], value_vars=value_vars)\n",
    "df_melt_table3['Instance size'] = df_melt_table3['level_0']\n",
    "df_melt_table3['Model'] = df_melt_table3['level_1']\n",
    "df_melt_table3.to_excel(os.path.join(outputfolder_table, '2_twct_model_stats_per_instance_no_hybrid.xlsx'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(df_table3.to_latex(index=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Now, we'll create a table with hybrid and non-hybrid solution methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = df_rpfs.copy().reset_index()\n",
    "### df_baseline = df_baseline[(df_baseline['optimal'] == True) & (df_baseline['validated'] == True)]\n",
    "exclude_model_list = ['Wilson']   ### ['Wilson-Hybrid', 'Liao-You-Hybrid', 'Manne-Hybrid', 'Wilson']\n",
    "df_baseline = df_baseline[~(df_baseline['model'].isin(exclude_model_list))]\n",
    "df_baseline = df_baseline[(df_baseline['validated'] == True) & (df_baseline['wct_validation'] > 0)]\n",
    "group_key = ['n', 'm', 'alpha', 'seq', 'RobCost_Gamma', 'instance_type']  #   , 'ub_name', 'instance_name']\n",
    "df_baseline_opt_value = df_baseline.groupby(by=group_key)[['wct_validation']].min().reset_index()\n",
    "display(df_baseline_opt_value)\n",
    "merge_key = group_key + ['wct_validation']\n",
    "df_baseline_opt = df_baseline.merge(df_baseline_opt_value, on=merge_key, how='inner')\n",
    "df_baseline_opt = df_baseline_opt.drop_duplicates(subset=merge_key, keep='first').sort_values(by=merge_key)\n",
    "display(df_baseline_opt)\n",
    "#df_baseline_opt.to_csv(os.path.join(outputfolder_table, 'BestOptimalSolutions.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Grouped by instance size, with hybrid method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perc_valid_solutions(df, model, instance_type, instance_size):\n",
    "    df_base = df.reset_index()\n",
    "    df_base = df_base[(df_base['instance_type'] == instance_type)]\n",
    "    df_base = df_base[(df_base['instance_size'].str.strip() == instance_size)]\n",
    "    df_model = df_base[df_base['model'] == model]\n",
    "    df_model_opt = df_model[(df_model['optimal'] == True) & (df_model['validated'] == True)]\n",
    "    df_join = df_baseline_opt.merge(df_model_opt, on=group_key, how='inner')\n",
    "    num_solutions = len(df_join.index)\n",
    "    df_join['obj_diff'] = df_join['wct_validation_y'] - df_join['wct_validation_x']\n",
    "    EPS = 1e-2\n",
    "    df_join['obj_less_than'] = df_join['obj_diff'] < -EPS\n",
    "    df_join['obj_greater_than'] = df_join['obj_diff'] > EPS\n",
    "    df_join['is_valid'] = ((abs(df_join['obj_diff']) <=EPS) | ( df_join['obj_less_than'] & (df_join['permutation_x'].str.strip() == df_join['permutation_y'].str.strip())))\n",
    "    df_join.to_csv(os.path.join(outputfolder_table, '2_ValidSolutions_{}_{}_{}.csv'.format(model, instance_type, instance_size)))\n",
    "    df_valid = df_join[df_join['is_valid']]\n",
    "    if num_solutions > 0:\n",
    "        return 100.0 * len(df_valid.index) / num_solutions\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_instance_stats = dict()\n",
    "for instance_type in instance_type_list:  # group by instance type and size\n",
    "    df_base = df_rpfs.reset_index()\n",
    "    df_itype = df_base[(df_base['instance_type'] == instance_type)]\n",
    "    instance_size_list = df_itype['instance_size'].unique().tolist()   # ['10x2', '10x3', '10x4', '10x5', '15x5']\n",
    "    for instance_size in instance_size_list:\n",
    "        df_instance = df_itype[df_itype['instance_size'] == instance_size]\n",
    "        for model in model_list:\n",
    "            per_instance_stats[(instance_type,instance_size,model)] = dict()\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% Best Performance'] = calculate_perc_best_performance(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% Solved'] = calculate_perc_solved(df_rpfs, model, instance_type, instance_size)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. % gap'] = calculate_avg_perc_gap(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of % gap'] = calculate_std(df_instance, model, 'gap')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['95% CI of % gap'] = mean_confidence_interval(df_instance, model, 'gap')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. time'] = calculate_avg_time(df_instance, model, 'time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of time'] = calculate_std(df_instance, model, 'time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. MP time'] = calculate_avg_time(df_instance, model, 'mp_total_time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. SP time'] = calculate_avg_time(df_instance, model, 'sp_total_time')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Avg. iterations'] = calculate_avg_iterations(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['Std. dev. of iterations'] = calculate_std(df_instance, model, 'iterations')\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['# instances solved'] = calculate_num_instances(df_instance, model)\n",
    "            per_instance_stats[(instance_type,instance_size,model)]['% valid solutions'] = calculate_perc_valid_solutions(df_base, model, instance_type, instance_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57606801/pandas-style-options-to-latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y, z) for (x, y, z) in per_instance_stats.keys() if (x == 'ying' and y in ['10x2', '10x3'])]\n",
    "per_instance_stats1 = { your_key: per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table3a = pd.DataFrame.from_dict(per_instance_stats1)\n",
    "df_table3a.columns = df_table3a.columns.droplevel()\n",
    "df_table3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y, z) for (x, y, z) in per_instance_stats.keys() if (x == 'ying' and y in ['10x4', '10x5'])]\n",
    "per_instance_stats2 = { your_key: per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table3b = pd.DataFrame.from_dict(per_instance_stats2)\n",
    "df_table3b.columns = df_table3b.columns.droplevel()\n",
    "df_table3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "allowed_keys = [(x, y, z) for (x, y, z) in per_instance_stats.keys() if (x == 'ying' and y in ['15x5'])]\n",
    "per_instance_stats3 = { your_key: per_instance_stats[your_key] for your_key in allowed_keys }\n",
    "df_table3c = pd.DataFrame.from_dict(per_instance_stats3)\n",
    "#df_table3c.columns = df_table3b.columns.droplevel()\n",
    "df_table3c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export table to Tableau, after melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table3 = pd.DataFrame.from_dict(per_instance_stats)\n",
    "df_table3.columns = df_table3.columns.droplevel()\n",
    "value_vars = df_table3.transpose().columns\n",
    "df_melt_table3 = pd.melt(df_table3.transpose().reset_index(), id_vars=['level_0', 'level_1'], value_vars=value_vars)\n",
    "df_melt_table3['Instance size'] = df_melt_table3['level_0']\n",
    "df_melt_table3['Model'] = df_melt_table3['level_1']\n",
    "df_melt_table3.to_excel(os.path.join(outputfolder_table, '2_twct_model_stats_per_instance_with_hybrid.xlsx'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
